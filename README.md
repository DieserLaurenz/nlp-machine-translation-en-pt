# NLP Project 2: English-Portuguese Machine Translation

This project was developed by **Laurenz Gilbert** and **Ariana Sahitaj** as part of their studies in Information Systems Management at the Technical University of Berlin, advised by Dr. Salar Mohtaj.

## Project Description

The goal of this project was to build a complete machine translation system for English to Portuguese using a sequence-to-sequence architecture with LSTM units.

The work included:
* An in-depth exploration of the Europarl corpus to identify issues like misalignments and vocabulary imbalances.
* Applying preprocessing steps such as lowercasing, token normalization, and sentence length filtering.
* Implementing several models, including baseline LSTMs with random, GloVe, and Word2Vec embeddings, as well as a character-based model.
* Evaluating all systems using BLEU and METEOR scores.
* Integrating an attention mechanism to improve performance, particularly on longer sentences.
* Exploring a pivot translation pipeline from Portuguese to Swedish, using English as an intermediate language.

## Important: Execution Environment

**This project was designed and trained on Kaggle** to leverage the free access to high-performance GPUs, which were not available privately. The file paths within the Jupyter Notebook (`nlp-projekt_gilbert_sahitaj.ipynb`) point to the Kaggle environment's dataset locations and **will not run on a local machine without modification.**

For full reproducibility and to execute the models in their intended environment, please use the official Kaggle Notebook:

**[Link to the Kaggle Notebook](https://www.kaggle.com/code/laurenzgilbert/nlp-project2-gilbert-sahitaj)**

## Setup and Dependencies

### 1. Python Libraries
The required Python libraries for this project are listed in the `requirements.txt` file. You can install them using pip:
```bash
pip install -r requirements.txt
```

### 2. Required Datasets (Manual Download)
The large data files required for this project are **not included** in this repository due to their size. You will need to download them manually.

* **Europarl Corpus**: The parallel corpora for English-Portuguese and English-Swedish need to be downloaded from the official source at [statmt.org](https://www.statmt.org/europarl/).
* **GloVe Embeddings**: This project uses the `glove.6B.100d.txt` file. It can be downloaded from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/).
* **Word2Vec Embeddings**: This project uses Google's pre-trained model (`GoogleNews-vectors-negative300.bin`). It can be downloaded from various sources online.

After downloading, you should place them in the `data/` directory according to the project structure described below.

## Project Structure

Here is an overview of the project's directory structure:

```
.
├── data/
│   ├── europarl-datasets/
│   │   ├── europarl-v7.pt-en.en
│   │   ├── europarl-v7.pt-en.pt
│   │   ├── europarl-v7.sv-en.en
│   │   ├── europarl-v7.sv-en.sv
│   │   ├── preprocessed_accurate.parquet
│   │   └── preprocessed_sv_en.parquet
│   │
│   ├── GloVe/
│   │   └── (glove.6B.100d.txt)  <-- Must be downloaded manually
│   │
│   └── Word2Vec/
│       └── (GoogleNews-vectors-negative300.bin)  <-- Must be downloaded manually
│
├── output/
│   ├── hypotheses_cache/
│   │   ├── Attention_Model_(ENtoPT)_1k.json
│   │   └── ... (other model hypotheses)
│   │
│   └── weights/
│       ├── attention_model_weights.weights.h5
│       └── ... (other model weights)
│
├── .gitignore
├── nlp-projekt_gilbert_sahitaj.ipynb
├── Project2_Gilbert_Sahitaj.pdf
└── README.md
```

### File Descriptions

* **`nlp-projekt_gilbert_sahitaj.ipynb`**: The main Jupyter Notebook containing all the code for data preprocessing, model implementation, training, and evaluation.
* **`Project2_Gilbert_Sahitaj.pdf`**: The final project report, which details the methodology, results, and conclusions of the study.
* **`README.md`**: This file, providing a comprehensive overview of the project.
* **`requirements.txt`**: A list of Python packages required to run the notebook.
* **`.gitignore`**: Specifies which files and folders (like the `data/` and `output/` directories) should be ignored by Git. This is crucial for preventing large files from being pushed to the repository.
* **`data/`**: This directory contains all data needed for the project. **Note:** This folder should be created manually and populated with the datasets as described above.
    * `europarl-datasets/`: Contains the raw and preprocessed `.parquet` files from the Europarl corpus.
    * `GloVe/` & `Word2Vec/`: These folders are intended to hold the pre-trained word embedding files, which **must be downloaded separately**.
* **`output/`**: This directory stores all artifacts generated by the notebook. It is excluded from the repository by the `.gitignore` file.
    * `hypotheses_cache/`: Contains JSON files with the translated sentences (hypotheses) generated by each model during testing.
    * `weights/`: Contains the trained model parameters saved as `.h5` files, allowing for model reuse without retraining.

## Authors

* **Laurenz Gilbert** ([gilbert@campus.tu-berlin.de](mailto:gilbert@campus.tu-berlin.de))
* **Ariana Sahitaj** ([ariana.sahitaj@campus.tu-berlin.de](mailto:ariana.sahitaj@campus.tu-berlin.de))