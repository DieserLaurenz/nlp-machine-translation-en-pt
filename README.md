# NLP Project 2: English-Portuguese Machine Translation

This project was developed by **Laurenz Gilbert** and **Ariana Sahitaj** as part of their studies in Information Systems Management at the Technical University of Berlin, advised by Dr. Salar Mohtaj.

## Project Description

The goal of this project was to build a complete machine translation system for English to Portuguese using a sequence-to-sequence architecture with LSTM units.

The work included:
* An in-depth exploration of the Europarl corpus to identify issues like misalignments and vocabulary imbalances.
* Applying preprocessing steps such as lowercasing, token normalization, and sentence length filtering.
* Implementing several models, including baseline LSTMs with random, GloVe, and Word2Vec embeddings, as well as a character-based model.
* Evaluating all systems using BLEU and METEOR scores.
* Integrating an attention mechanism to improve performance, particularly on longer sentences.
* Exploring a pivot translation pipeline from Portuguese to Swedish, using English as an intermediate language.

## Important: Execution Environment

**This project was designed and trained on Kaggle** to leverage the free access to high-performance GPUs, which were not available privately. The file paths within the Jupyter Notebook (`nlp-projekt_gilbert_sahitaj.ipynb`) point to the Kaggle environment's dataset locations and **will not run on a local machine without modification.**

For full reproducibility and to execute the models in their intended environment, please use the official Kaggle Notebook:

**[Link to the Kaggle Notebook](https://www.kaggle.com/code/laurenzgilbert/nlp-project2-gilbert-sahitaj)**

## Setup and Dependencies

### 1. Python Libraries
The required Python libraries for this project are listed in the `requirements.txt` file. You can install them using pip:
```bash
pip install -r requirements.txt
```

### 2. Required Datasets (Manual Download)
The large data files required for this project are **not included** in this repository due to their size. You will need to download them manually.

* **Europarl Corpus**: The parallel corpora for English-Portuguese and English-Swedish need to be downloaded from the official source at [statmt.org](https://www.statmt.org/europarl/).
* **GloVe Embeddings**: This project uses the `glove.6B.100d.txt` file. It can be downloaded from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/).
* **Word2Vec Embeddings**: This project uses Google's pre-trained model (`GoogleNews-vectors-negative300.bin`). It can be downloaded from various sources online.

After downloading, you should place them in the `data/` directory according to the project structure described below.

## Project Structure

Here is an overview of the project's directory structure:

**Please note:** The `data/` and `output/` directories are **not included** in this repository due to their very large file sizes. The complete data and the resulting models are available in the linked Kaggle environment.

```
.
├── data/                  <-- Not included in this repository
│   ├── europarl-datasets/
│   │   ├── europarl-v7.pt-en.en
│   │   └── ... (other language pairs)
│   ├── GloVe/
│   │   └── glove.6B.100d.txt
│   └── Word2Vec/
│       └── GoogleNews-vectors-negative300.bin
│
├── output/                <-- Not included in this repository
│   ├── hypotheses_cache/
│   │   └── ... (translation outputs in .json)
│   └── weights/
│       ├── attention_model_weights.weights.h5
│       └── ... (other model weights)
│
├── .gitignore
├── nlp-projekt_gilbert_sahitaj.ipynb
├── Project2_Gilbert_Sahitaj.pdf
└── README.md
```

### File Descriptions

* **`nlp-projekt_gilbert_sahitaj.ipynb`**: The main Jupyter Notebook containing all code for data preprocessing, model implementation, training, and evaluation.
* **`Project2_Gilbert_Sahitaj.pdf`**: The final project report, detailing the methodology, results, and conclusions.
* **`README.md`**: This file.
* **`.gitignore`**: Specifies which files and folders (like `data/` and `output/`) should be ignored by Git.

---
#### Contents of `data/` (Not in Repository)
This directory contains the datasets required to run the project.

* **`europarl-datasets/`**: Holds the raw parallel text corpora.
    * `europarl-v7.pt-en.en` & `...pt`: The English-Portuguese sentence pairs from the Europarl v7 corpus.
* **`GloVe/`**: Contains pre-trained GloVe word embeddings.
    * `glove.6B.100d.txt`: The 100-dimensional GloVe vectors trained on 6 billion tokens.
* **`Word2Vec/`**: Contains the pre-trained Word2Vec model.
    * `GoogleNews-vectors-negative300.bin`: Google's powerful 300-dimensional Word2Vec model trained on Google News data.

---
#### Contents of `output/` (Not in Repository)
This directory stores all artifacts generated by the notebook.

* **`hypotheses_cache/`**: Contains JSON files with the translated sentences (hypotheses) generated by each model during testing.
* **`weights/`**: Contains the trained model parameters, allowing for model reuse without retraining.
    * `attention_model_weights.weights.h5`, `base_model_weights.weights.h5`, etc.: Each `.h5` file stores the learned weights for a specific model implemented in the notebook.

## Authors

* **Laurenz Gilbert** ([gilbert@campus.tu-berlin.de](mailto:gilbert@campus.tu-berlin.de))
* **Ariana Sahitaj** ([ariana.sahitaj@campus.tu-berlin.de](mailto:ariana.sahitaj@campus.tu-berlin.de))
